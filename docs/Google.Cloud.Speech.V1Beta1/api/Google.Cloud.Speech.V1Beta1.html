<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Namespace Google.Cloud.Speech.V1Beta1
   | Google.Cloud.Speech.V1Beta1 </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Namespace Google.Cloud.Speech.V1Beta1
   | Google.Cloud.Speech.V1Beta1 ">
    <meta name="generator" content="docfx 2.5.4.0">
    
    <link rel="shortcut icon" href="../favicon.ico">
    <link rel="stylesheet" href="../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../styles/docfx.css">
    <link rel="stylesheet" href="../styles/main.css">
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
  </head>
  <body data-spy="scroll" data-target="#affix">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              <a class="navbar-brand" href="../index.html">
                <img id="logo" class="svg" src="../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content">
              
              <h1 id="Google_Cloud_Speech_V1Beta1" data-uid="Google.Cloud.Speech.V1Beta1">Namespace Google.Cloud.Speech.V1Beta1
              </h1>
              <div class="markdown level0 summary"></div>
              <div class="markdown level0 conceptual"></div>
              <div class="markdown level0 remarks"></div>
                <h3 id="classes">Classes
              </h3>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.AsyncRecognizeMetadata.html#Google_Cloud_Speech_V1Beta1_AsyncRecognizeMetadata">AsyncRecognizeMetadata</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.AsyncRecognizeMetadata.yml" sourcestartlinenumber="2" sourceendlinenumber="5"> <code>AsyncRecognizeMetadata</code> describes the progress of a long-running
 <code>AsyncRecognize</code> call. It is included in the <code>metadata</code> field of the
 <code>Operation</code> returned by the <code>GetOperation</code> call of the
 <code>google::longrunning::Operations</code> service.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.AsyncRecognizeRequest.html#Google_Cloud_Speech_V1Beta1_AsyncRecognizeRequest">AsyncRecognizeRequest</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.AsyncRecognizeRequest.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> <code>AsyncRecognizeRequest</code> is the top-level message sent by the client for
 the <code>AsyncRecognize</code> method.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.AsyncRecognizeResponse.html#Google_Cloud_Speech_V1Beta1_AsyncRecognizeResponse">AsyncRecognizeResponse</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.AsyncRecognizeResponse.yml" sourcestartlinenumber="2" sourceendlinenumber="6"> <code>AsyncRecognizeResponse</code> is the only message returned to the client by
 <code>AsyncRecognize</code>. It contains the result as zero or more sequential
 <code>SpeechRecognitionResult</code> messages. It is included in the <code>result.response</code>
 field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the
 <code>google::longrunning::Operations</code> service.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.RecognitionAudio.html#Google_Cloud_Speech_V1Beta1_RecognitionAudio">RecognitionAudio</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.RecognitionAudio.yml" sourcestartlinenumber="2" sourceendlinenumber="5"> Contains audio data in the encoding specified in the <code>RecognitionConfig</code>.
 Either <code>content</code> or <code>uri</code> must be supplied. Supplying both or neither
 returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
 <a href="https://cloud.google.com/speech/limits#content" sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.RecognitionAudio.yml" sourcestartlinenumber="5" sourceendlinenumber="5">audio limits</a>.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.RecognitionConfig.html#Google_Cloud_Speech_V1Beta1_RecognitionConfig">RecognitionConfig</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.RecognitionConfig.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> The <code>RecognitionConfig</code> message provides information to the recognizer
 that specifies how to process the request.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.Speech.html#Google_Cloud_Speech_V1Beta1_Speech">Speech</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.Speech.yml" sourcestartlinenumber="2" sourceendlinenumber="2"> Service that implements Google Cloud Speech API.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.Speech.SpeechBase.html#Google_Cloud_Speech_V1Beta1_Speech_SpeechBase">Speech.SpeechBase</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.Speech.SpeechBase.yml" sourcestartlinenumber="1" sourceendlinenumber="1">Base class for server-side implementations of Speech</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.Speech.SpeechClient.html#Google_Cloud_Speech_V1Beta1_Speech_SpeechClient">Speech.SpeechClient</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.Speech.SpeechClient.yml" sourcestartlinenumber="1" sourceendlinenumber="1">Client for Speech</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechClient.html#Google_Cloud_Speech_V1Beta1_SpeechClient">SpeechClient</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechClient.yml" sourcestartlinenumber="2" sourceendlinenumber="2">Speech client wrapper, for convenient use.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechClientImpl.html#Google_Cloud_Speech_V1Beta1_SpeechClientImpl">SpeechClientImpl</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechClientImpl.yml" sourcestartlinenumber="2" sourceendlinenumber="2">Speech client wrapper implementation, for convenient use.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechContext.html#Google_Cloud_Speech_V1Beta1_SpeechContext">SpeechContext</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechContext.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases
 in the results.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechRecognitionAlternative.html#Google_Cloud_Speech_V1Beta1_SpeechRecognitionAlternative">SpeechRecognitionAlternative</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechRecognitionAlternative.yml" sourcestartlinenumber="2" sourceendlinenumber="2"> Alternative hypotheses (a.k.a. n-best list).</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechRecognitionResult.html#Google_Cloud_Speech_V1Beta1_SpeechRecognitionResult">SpeechRecognitionResult</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechRecognitionResult.yml" sourcestartlinenumber="2" sourceendlinenumber="2"> A speech recognition result corresponding to a portion of the audio.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechSettings.html#Google_Cloud_Speech_V1Beta1_SpeechSettings">SpeechSettings</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SpeechSettings.yml" sourcestartlinenumber="2" sourceendlinenumber="2">Settings for a <a class="xref" href="Google.Cloud.Speech.V1Beta1.SpeechClient.html#Google_Cloud_Speech_V1Beta1_SpeechClient">SpeechClient</a>.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.StreamingRecognitionConfig.html#Google_Cloud_Speech_V1Beta1_StreamingRecognitionConfig">StreamingRecognitionConfig</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognitionConfig.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> The <code>StreamingRecognitionConfig</code> message provides information to the
 recognizer that specifies how to process the request.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.StreamingRecognitionResult.html#Google_Cloud_Speech_V1Beta1_StreamingRecognitionResult">StreamingRecognitionResult</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognitionResult.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> A streaming speech recognition result corresponding to a portion of the audio
 that is currently being processed.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.StreamingRecognizeRequest.html#Google_Cloud_Speech_V1Beta1_StreamingRecognizeRequest">StreamingRecognizeRequest</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeRequest.yml" sourcestartlinenumber="2" sourceendlinenumber="6"> <code>StreamingRecognizeRequest</code> is the top-level message sent by the client for
 the <code>StreamingRecognize</code>. Multiple <code>StreamingRecognizeRequest</code> messages are
 sent. The first message must contain a <code>streaming_config</code> message and must
 not contain <code>audio</code> data. All subsequent messages must contain <code>audio</code> data
 and must not contain a <code>streaming_config</code> message.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.html#Google_Cloud_Speech_V1Beta1_StreamingRecognizeResponse">StreamingRecognizeResponse</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="2" sourceendlinenumber="4"> <code>StreamingRecognizeResponse</code> is the only message returned to the client by
 <code>StreamingRecognize</code>. A series of one or more <code>StreamingRecognizeResponse</code>
 messages are streamed back to the client.</p>
<p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="6" sourceendlinenumber="7"> Here&#39;s an example of a series of ten <code>StreamingRecognizeResponse</code>s that might
 be returned while processing audio:</p>
<ol sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="9" sourceendlinenumber="58">
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="9" sourceendlinenumber="9"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="9" sourceendlinenumber="9">endpointer_type: START_OF_SPEECH</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="11" sourceendlinenumber="12"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="11" sourceendlinenumber="12">results { alternatives { transcript: &quot;tube&quot; } stability: 0.01 }
result_index: 0</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="14" sourceendlinenumber="15"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="14" sourceendlinenumber="15">results { alternatives { transcript: &quot;to be a&quot; } stability: 0.01 }
result_index: 0</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="17" sourceendlinenumber="19"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="17" sourceendlinenumber="19">results { alternatives { transcript: &quot;to be&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; or not to be&quot; } stability: 0.01 }
result_index: 0</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="21" sourceendlinenumber="25"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="21" sourceendlinenumber="21">results { alternatives { transcript: &quot;to be or not to be&quot;</p>
<pre sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="22" sourceendlinenumber="24"><code>                     confidence: 0.92 }
      alternatives { transcript: &quot;to bee or not to bee&quot; }
      is_final: true }
</code></pre><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="21" sourceendlinenumber="21">result_index: 0</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="27" sourceendlinenumber="28"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="27" sourceendlinenumber="28">results { alternatives { transcript: &quot; that&#39;s&quot; } stability: 0.01 }
result_index: 1</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="30" sourceendlinenumber="32"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="30" sourceendlinenumber="32">results { alternatives { transcript: &quot; that is&quot; } stability: 0.9 }
results { alternatives { transcript: &quot; the question&quot; } stability: 0.01 }
result_index: 1</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="34" sourceendlinenumber="34"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="34" sourceendlinenumber="34">endpointer_type: END_OF_SPEECH</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="36" sourceendlinenumber="40"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="36" sourceendlinenumber="36">results { alternatives { transcript: &quot; that is the question&quot;</p>
<pre sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="37" sourceendlinenumber="39"><code>                     confidence: 0.98 }
      alternatives { transcript: &quot; that was the question&quot; }
      is_final: true }
</code></pre><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="36" sourceendlinenumber="36">result_index: 1</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="42" sourceendlinenumber="44"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="42" sourceendlinenumber="42">endpointer_type: END_OF_AUDIO</p>
<p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="42" sourceendlinenumber="42">Notes:</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="46" sourceendlinenumber="48"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="46" sourceendlinenumber="48">Only two of the above responses #5 and #9 contain final results, they are
indicated by <code>is_final: true</code>. Concatenating these together generates the
full transcript: &quot;to be or not to be that is the question&quot;.</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="50" sourceendlinenumber="53"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="50" sourceendlinenumber="53">The others contain interim <code>results</code>. #4 and #7 contain two interim
<code>results</code>, the first portion has a high stability and is less likely to
change, the second portion has a low stability and is very likely to
change. A UI designer might choose to show only high stability <code>results</code>.</p>
</li>
<li sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="55" sourceendlinenumber="58"><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeResponse.yml" sourcestartlinenumber="55" sourceendlinenumber="58">The <code>result_index</code> indicates the portion of audio that has had final
results returned, and is no longer being processed. For example, the
<code>results</code> in #6 and later correspond to the portion of audio after
&quot;to be or not to be&quot;.</p>
</li>
</ol>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SyncRecognizeRequest.html#Google_Cloud_Speech_V1Beta1_SyncRecognizeRequest">SyncRecognizeRequest</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SyncRecognizeRequest.yml" sourcestartlinenumber="2" sourceendlinenumber="3"> <code>SyncRecognizeRequest</code> is the top-level message sent by the client for
 the <code>SyncRecognize</code> method.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.SyncRecognizeResponse.html#Google_Cloud_Speech_V1Beta1_SyncRecognizeResponse">SyncRecognizeResponse</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.SyncRecognizeResponse.yml" sourcestartlinenumber="2" sourceendlinenumber="4"> <code>SyncRecognizeResponse</code> is the only message returned to the client by
 <code>SyncRecognize</code>. It contains the result as zero or more sequential
 <code>SpeechRecognitionResult</code> messages.</p>
</section>
                <h3 id="enums">Enums
              </h3>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.RecognitionAudio.AudioSourceOneofCase.html#Google_Cloud_Speech_V1Beta1_RecognitionAudio_AudioSourceOneofCase">RecognitionAudio.AudioSourceOneofCase</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.RecognitionAudio.AudioSourceOneofCase.yml" sourcestartlinenumber="1" sourceendlinenumber="1">Enum of possible cases for the &quot;audio_source&quot; oneof.</p>
</section>
                  <h4><a class="xref" href="Google.Cloud.Speech.V1Beta1.StreamingRecognizeRequest.StreamingRequestOneofCase.html#Google_Cloud_Speech_V1Beta1_StreamingRecognizeRequest_StreamingRequestOneofCase">StreamingRecognizeRequest.StreamingRequestOneofCase</a></h4>
                  <section><p sourcefile="obj/api/Google.Cloud.Speech.V1Beta1.StreamingRecognizeRequest.StreamingRequestOneofCase.yml" sourcestartlinenumber="1" sourceendlinenumber="1">Enum of possible cases for the &quot;streaming_request&quot; oneof.</p>
</section>
            </article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
             
            
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../styles/docfx.vendor.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
